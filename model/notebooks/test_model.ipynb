{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b2d9ef",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 13:42:43,953 - INFO - Sentry DSN set to: https://f4f21cc936b3ba9f5dbc1464b7a40ea4@o4504168838070272.ingest.us.sentry.io/4506464560414720\n",
      "2025-10-22 13:42:43,954 - INFO - Sentry initialized with environment: development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config...\n",
      "Loading secrets...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "from settings.global_settings import GlobalSettings\n",
    "\n",
    "config = GlobalSettings.get_config(\n",
    "    config_file = \"../config.ini\",\n",
    "    secrets_file = \"../secrets.ini\"\n",
    ")\n",
    "from dataset.video_loader import VideoDataLoader\n",
    "from dataset.video_dataset import VideoDataset\n",
    "from model.training_loop import train\n",
    "from torch.utils.data import random_split\n",
    "from model.multimodal_har_model import MultiModalHARModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf9b62",
   "metadata": {},
   "source": [
    "## Initializing Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d691d46",
   "metadata": {},
   "source": [
    "**Creating Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d84473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 13:42:45,917 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-10-22 13:42:46,144 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-10-22 13:42:46,452 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-10-22 13:42:46,773 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-10-22 13:42:47,180 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-10-22 13:42:47,395 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-10-22 13:42:47,621 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-10-22 13:42:48,196 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-10-22 13:42:48,676 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-10-22 13:42:48,908 - INFO - [VideoDataLoader] Loding action videos for action: a12\n",
      "2025-10-22 13:42:53,692 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-10-22 13:42:53,756 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-10-22 13:42:53,816 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-10-22 13:42:53,871 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-10-22 13:42:53,932 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-10-22 13:42:53,988 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-10-22 13:42:54,042 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-10-22 13:42:54,165 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-10-22 13:42:54,256 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-10-22 13:42:54,312 - INFO - [VideoDataLoader] Loding action videos for action: a12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"train\"\n",
    ")\n",
    "TEST_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"test\"\n",
    ")\n",
    "\n",
    "train_video_data_loader = VideoDataLoader(\n",
    "    path=TRAIN_DIR,\n",
    ")\n",
    "test_video_data_loader = VideoDataLoader(\n",
    "    path=TEST_DIR,\n",
    ")\n",
    "\n",
    "train_dataset = VideoDataset(\n",
    "    video_data_loader=train_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "test_dataset = VideoDataset(\n",
    "    video_data_loader=test_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "\n",
    "len(train_dataset)\n",
    "for _ in train_dataset:\n",
    "    pass\n",
    "len(train_dataset.labels_map)\n",
    "\n",
    "len(test_dataset)\n",
    "for _ in test_dataset:\n",
    "    pass\n",
    "len(test_dataset.labels_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5336f",
   "metadata": {},
   "source": [
    "**Splitting Train and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c82257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_total = len(train_dataset)\n",
    "# num_train = int(0.8 * num_total)\n",
    "# num_test = num_total - num_train\n",
    "# train_dataset, test_dataset = random_split(train_dataset, [num_train, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef613137",
   "metadata": {},
   "source": [
    "**Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781f5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "har_model = MultiModalHARModel(\n",
    "    obj_in=train_dataset[0].graphs_objects[0].x.shape[1],\n",
    "    joint_in=train_dataset[0].graphs_joints[0].x.shape[1],\n",
    "    gat_hidden=128,\n",
    "    gat_out=128,\n",
    "    temporal_hidden=128,\n",
    "    num_classes=len(train_dataset.labels_map), \n",
    "    dropout=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cabcd9",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37b4ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 13:42:55,262 - INFO - Starting training loop...\n",
      "Epoch 1/25: 100%|██████████| 1176/1176 [06:55<00:00,  2.83it/s]\n",
      "2025-10-22 13:49:51,176 - INFO - Epoch 1/25, Loss: 1.8997\n",
      "Epoch 2/25: 100%|██████████| 1176/1176 [06:37<00:00,  2.96it/s]\n",
      "2025-10-22 13:56:28,396 - INFO - Epoch 2/25, Loss: 1.4471\n",
      "Epoch 3/25: 100%|██████████| 1176/1176 [06:38<00:00,  2.95it/s]\n",
      "2025-10-22 14:03:07,197 - INFO - Epoch 3/25, Loss: 1.2742\n",
      "Epoch 4/25: 100%|██████████| 1176/1176 [06:37<00:00,  2.96it/s]\n",
      "2025-10-22 14:09:44,244 - INFO - Epoch 4/25, Loss: 1.1266\n",
      "Epoch 5/25: 100%|██████████| 1176/1176 [33:07<00:00,  1.69s/it]   \n",
      "2025-10-22 14:42:51,606 - INFO - Epoch 5/25, Loss: 1.0480\n",
      "Epoch 6/25: 100%|██████████| 1176/1176 [06:38<00:00,  2.95it/s]\n",
      "2025-10-22 14:49:30,002 - INFO - Epoch 6/25, Loss: 0.9766\n",
      "Epoch 7/25: 100%|██████████| 1176/1176 [06:39<00:00,  2.94it/s]\n",
      "2025-10-22 14:56:09,587 - INFO - Epoch 7/25, Loss: 0.9123\n",
      "Epoch 8/25: 100%|██████████| 1176/1176 [07:04<00:00,  2.77it/s]\n",
      "2025-10-22 15:03:14,325 - INFO - Epoch 8/25, Loss: 0.8428\n",
      "Epoch 9/25: 100%|██████████| 1176/1176 [07:19<00:00,  2.68it/s]\n",
      "2025-10-22 15:10:33,687 - INFO - Epoch 9/25, Loss: 0.7333\n",
      "Epoch 10/25: 100%|██████████| 1176/1176 [07:09<00:00,  2.74it/s]\n",
      "2025-10-22 15:17:42,854 - INFO - Epoch 10/25, Loss: 0.7184\n",
      "Epoch 11/25: 100%|██████████| 1176/1176 [07:09<00:00,  2.74it/s]\n",
      "2025-10-22 15:24:52,034 - INFO - Epoch 11/25, Loss: 0.6343\n",
      "Epoch 12/25: 100%|██████████| 1176/1176 [07:00<00:00,  2.80it/s]\n",
      "2025-10-22 15:31:52,347 - INFO - Epoch 12/25, Loss: 0.5804\n",
      "Epoch 13/25: 100%|██████████| 1176/1176 [06:34<00:00,  2.98it/s]\n",
      "2025-10-22 15:38:26,800 - INFO - Epoch 13/25, Loss: 0.5529\n",
      "Epoch 14/25: 100%|██████████| 1176/1176 [06:30<00:00,  3.01it/s]\n",
      "2025-10-22 15:44:57,439 - INFO - Epoch 14/25, Loss: 0.5158\n",
      "Epoch 15/25: 100%|██████████| 1176/1176 [06:39<00:00,  2.95it/s]\n",
      "2025-10-22 15:51:36,750 - INFO - Epoch 15/25, Loss: 0.4958\n",
      "Epoch 16/25: 100%|██████████| 1176/1176 [06:41<00:00,  2.93it/s]\n",
      "2025-10-22 15:58:17,799 - INFO - Epoch 16/25, Loss: 0.4222\n",
      "Epoch 17/25: 100%|██████████| 1176/1176 [06:43<00:00,  2.91it/s]\n",
      "2025-10-22 16:05:01,450 - INFO - Epoch 17/25, Loss: 0.3388\n",
      "Epoch 18/25: 100%|██████████| 1176/1176 [06:44<00:00,  2.90it/s]\n",
      "2025-10-22 16:11:46,388 - INFO - Epoch 18/25, Loss: 0.3689\n",
      "Epoch 19/25: 100%|██████████| 1176/1176 [06:50<00:00,  2.86it/s]\n",
      "2025-10-22 16:18:37,306 - INFO - Epoch 19/25, Loss: 0.3497\n",
      "Epoch 20/25: 100%|██████████| 1176/1176 [06:37<00:00,  2.96it/s]\n",
      "2025-10-22 16:25:15,024 - INFO - Epoch 20/25, Loss: 0.3327\n",
      "Epoch 21/25: 100%|██████████| 1176/1176 [06:43<00:00,  2.92it/s]\n",
      "2025-10-22 16:31:58,321 - INFO - Epoch 21/25, Loss: 0.3026\n",
      "Epoch 22/25: 100%|██████████| 1176/1176 [06:40<00:00,  2.93it/s]\n",
      "2025-10-22 16:38:39,094 - INFO - Epoch 22/25, Loss: 0.3266\n",
      "Epoch 23/25: 100%|██████████| 1176/1176 [07:42<00:00,  2.54it/s] \n",
      "2025-10-22 16:46:21,531 - INFO - Epoch 23/25, Loss: 0.2502\n",
      "Epoch 24/25: 100%|██████████| 1176/1176 [06:37<00:00,  2.96it/s]\n",
      "2025-10-22 16:52:59,154 - INFO - Epoch 24/25, Loss: 0.2900\n",
      "Epoch 25/25: 100%|██████████| 1176/1176 [21:30<00:00,  1.10s/it]   \n",
      "2025-10-22 17:14:29,449 - INFO - Epoch 25/25, Loss: 0.2733\n"
     ]
    }
   ],
   "source": [
    "train_history = train(\n",
    "    model=har_model,\n",
    "    video_dataset=train_dataset,\n",
    "    device='mps',\n",
    "    epochs=25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5bc8d",
   "metadata": {},
   "source": [
    "**Saving Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b74bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 17:14:29,583 - INFO - Saving model to /Volumes/KODAK/masters/model/validation_datasets/NW-UCLA/model/har_model_v0.0.0_nw_ucla_2025-10-22 17:14:29.583586.pht...\n",
      "2025-10-22 17:14:29,861 - INFO - Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "har_model.save(\n",
    "    training_history=train_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4056878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "model_settings = config.model_settings\n",
    "save_dir = \"~/masters-models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(\n",
    "    save_dir,\n",
    "    f\"har_model_{model_settings.model_version}_{model_settings.dataset_prefix}_100perc_{datetime.now()}.pht\"\n",
    ")\n",
    "torch.save({\n",
    "\"model_state_dict\": har_model.state_dict(),\n",
    "\"training_history\": train_history,\n",
    "\"model_config\": har_model._model_config,\n",
    "}, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf73a0",
   "metadata": {},
   "source": [
    "## Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f60c1",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a215b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            sample = dataset[i]\n",
    "            label = sample.label.to(device)\n",
    "\n",
    "            # Move all graph tensors to device\n",
    "            graphs_objects = [g.to(device) for g in sample.graphs_objects]\n",
    "            graphs_joints = [g.to(device) for g in sample.graphs_joints]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(graphs_objects, graphs_joints)\n",
    "\n",
    "            # Compute prediction\n",
    "            if output.dim() == 1:\n",
    "                predicted = torch.argmax(output).unsqueeze(0)\n",
    "            else:\n",
    "                _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "            correct += (predicted == label).sum().item()\n",
    "            total += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a102aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.24%\n"
     ]
    }
   ],
   "source": [
    "accuracy_evaluation = evaluate(har_model, test_dataset, \"mps\")\n",
    "print(f\"Test Accuracy: {accuracy_evaluation:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
