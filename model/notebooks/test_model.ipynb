{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b2d9ef",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 20:19:23,408 - INFO - Sentry DSN set to: https://f4f21cc936b3ba9f5dbc1464b7a40ea4@o4504168838070272.ingest.us.sentry.io/4506464560414720\n",
      "2025-10-25 20:19:23,409 - INFO - Sentry initialized with environment: development\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config...\n",
      "Loading secrets...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "from settings.global_settings import GlobalSettings\n",
    "\n",
    "config = GlobalSettings.get_config(\n",
    "    config_file = \"../config.ini\",\n",
    "    secrets_file = \"../secrets.ini\"\n",
    ")\n",
    "from dataset.video_loader import VideoDataLoader\n",
    "from dataset.video_dataset import VideoDataset\n",
    "from model.training_loop import train, EarlyStoppingParams\n",
    "from model.multimodal_har_model import MultiModalHARModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf9b62",
   "metadata": {},
   "source": [
    "## Initializing Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d691d46",
   "metadata": {},
   "source": [
    "**Creating Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d84473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 20:19:25,459 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-10-25 20:19:26,073 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-10-25 20:19:26,813 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-10-25 20:19:27,466 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-10-25 20:19:28,576 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-10-25 20:19:29,194 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-10-25 20:19:29,774 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-10-25 20:19:31,225 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-10-25 20:19:32,442 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-10-25 20:19:33,247 - INFO - [VideoDataLoader] Loding action videos for action: a12\n",
      "2025-10-25 20:19:38,367 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-10-25 20:19:38,395 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-10-25 20:19:38,430 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-10-25 20:19:38,456 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-10-25 20:19:38,485 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-10-25 20:19:38,510 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-10-25 20:19:38,700 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-10-25 20:19:38,760 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-10-25 20:19:38,972 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-10-25 20:19:38,999 - INFO - [VideoDataLoader] Loding action videos for action: a12\n",
      "2025-10-25 20:19:39,595 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-10-25 20:19:39,622 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-10-25 20:19:39,650 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-10-25 20:19:39,671 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-10-25 20:19:39,701 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-10-25 20:19:39,727 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-10-25 20:19:39,749 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-10-25 20:19:39,961 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-10-25 20:19:39,997 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-10-25 20:19:40,020 - INFO - [VideoDataLoader] Loding action videos for action: a12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"train\"\n",
    ")\n",
    "TEST_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"validation\"\n",
    ")\n",
    "VALIDATION_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"test\"\n",
    ")\n",
    "\n",
    "train_video_data_loader = VideoDataLoader(\n",
    "    path=TRAIN_DIR,\n",
    ")\n",
    "test_video_data_loader = VideoDataLoader(\n",
    "    path=TEST_DIR,\n",
    ")\n",
    "validation_video_data_loader = VideoDataLoader(\n",
    "    path=VALIDATION_DIR,\n",
    ")\n",
    "\n",
    "train_dataset = VideoDataset(\n",
    "    video_data_loader=train_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "test_dataset = VideoDataset(\n",
    "    video_data_loader=test_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "validation_dataset = VideoDataset(\n",
    "    video_data_loader=validation_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "\n",
    "len(train_dataset)\n",
    "for _ in train_dataset:\n",
    "    pass\n",
    "len(train_dataset.labels_map)\n",
    "\n",
    "len(test_dataset)\n",
    "for _ in test_dataset:\n",
    "    pass\n",
    "\n",
    "len(validation_dataset)\n",
    "for _ in validation_dataset:\n",
    "    pass\n",
    "\n",
    "\n",
    "display(len(test_dataset.labels_map))\n",
    "display(len(validation_dataset.labels_map))\n",
    "display(len(train_dataset.labels_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5336f",
   "metadata": {},
   "source": [
    "**Splitting Train and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c82257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_total = len(train_dataset)\n",
    "# num_train = int(0.8 * num_total)\n",
    "# num_test = num_total - num_train\n",
    "# train_dataset, test_dataset = random_split(train_dataset, [num_train, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef613137",
   "metadata": {},
   "source": [
    "**Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781f5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "har_model = MultiModalHARModel(\n",
    "    obj_in=train_dataset[0].graphs_objects[0].x.shape[1],\n",
    "    joint_in=train_dataset[0].graphs_joints[0].x.shape[1],\n",
    "    gat_hidden=128,\n",
    "    gat_out=128,\n",
    "    temporal_hidden=128,\n",
    "    num_classes=len(train_dataset.labels_map), \n",
    "    dropout=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8464c4",
   "metadata": {},
   "source": [
    "**Create Evaluate Function For Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a215b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    import torch\n",
    "    device = 'mps'\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            sample = dataset[i]\n",
    "            label = sample.label.to(device)\n",
    "\n",
    "            # Move all graph tensors to device\n",
    "            graphs_objects = [g.to(device) for g in sample.graphs_objects]\n",
    "            graphs_joints = [g.to(device) for g in sample.graphs_joints]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(graphs_objects, graphs_joints)\n",
    "\n",
    "            # Compute prediction\n",
    "            if output.dim() == 1:\n",
    "                predicted = torch.argmax(output).unsqueeze(0)\n",
    "            else:\n",
    "                _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "            correct += (predicted == label).sum().item()\n",
    "            total += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cabcd9",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2e009",
   "metadata": {},
   "source": [
    "**Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_params = EarlyStoppingParams(\n",
    "    patience=5,\n",
    "    min_delta=1e-4,\n",
    "    mode='max',\n",
    "    evaluation_function=evaluate,\n",
    "    evaluation_dataset=validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6abf28",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37b4ad38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 20:19:40,682 - INFO - Starting training loop...\n",
      "2025-10-25 20:19:40,722 - INFO - Using early stopping\n",
      "Epoch 1/25: 100%|██████████| 1176/1176 [06:40<00:00,  2.94it/s]\n",
      "2025-10-25 20:26:21,053 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 20:26:45,602 - INFO - Epoch 1/25, Loss: 1.9703\n",
      "Epoch 2/25: 100%|██████████| 1176/1176 [06:25<00:00,  3.05it/s]\n",
      "2025-10-25 20:33:11,205 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 20:33:32,582 - INFO - Epoch 2/25, Loss: 1.5095\n",
      "Epoch 3/25: 100%|██████████| 1176/1176 [06:26<00:00,  3.04it/s]\n",
      "2025-10-25 20:39:59,164 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 20:40:21,179 - INFO - Epoch 3/25, Loss: 1.3600\n",
      "Epoch 4/25: 100%|██████████| 1176/1176 [06:23<00:00,  3.07it/s]\n",
      "2025-10-25 20:46:44,329 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 20:47:04,970 - INFO - Epoch 4/25, Loss: 1.1955\n",
      "Epoch 5/25: 100%|██████████| 1176/1176 [06:05<00:00,  3.22it/s]\n",
      "2025-10-25 20:53:10,650 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 20:53:31,319 - INFO - Epoch 5/25, Loss: 1.1494\n",
      "Epoch 6/25: 100%|██████████| 1176/1176 [06:02<00:00,  3.25it/s]\n",
      "2025-10-25 20:59:33,465 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 20:59:53,945 - INFO - Epoch 6/25, Loss: 1.0742\n",
      "Epoch 7/25: 100%|██████████| 1176/1176 [06:00<00:00,  3.26it/s]\n",
      "2025-10-25 21:05:54,349 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:06:14,867 - INFO - Epoch 7/25, Loss: 1.0053\n",
      "Epoch 8/25: 100%|██████████| 1176/1176 [06:17<00:00,  3.12it/s]\n",
      "2025-10-25 21:12:32,166 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:12:53,739 - INFO - Epoch 8/25, Loss: 0.9079\n",
      "Epoch 9/25: 100%|██████████| 1176/1176 [06:30<00:00,  3.01it/s]\n",
      "2025-10-25 21:19:24,231 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:19:45,412 - INFO - Epoch 9/25, Loss: 0.8282\n",
      "Epoch 10/25: 100%|██████████| 1176/1176 [06:07<00:00,  3.20it/s]\n",
      "2025-10-25 21:25:52,995 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:26:14,371 - INFO - Epoch 10/25, Loss: 0.7764\n",
      "Epoch 11/25: 100%|██████████| 1176/1176 [05:59<00:00,  3.27it/s]\n",
      "2025-10-25 21:32:13,577 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:32:34,255 - INFO - Epoch 11/25, Loss: 0.7223\n",
      "Epoch 12/25: 100%|██████████| 1176/1176 [05:58<00:00,  3.28it/s]\n",
      "2025-10-25 21:38:33,172 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:38:53,699 - INFO - Epoch 12/25, Loss: 0.6844\n",
      "Epoch 13/25: 100%|██████████| 1176/1176 [06:02<00:00,  3.24it/s]\n",
      "2025-10-25 21:44:56,606 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:45:17,119 - INFO - Epoch 13/25, Loss: 0.6763\n",
      "Epoch 14/25: 100%|██████████| 1176/1176 [06:15<00:00,  3.13it/s]\n",
      "2025-10-25 21:51:33,001 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:51:54,533 - INFO - Epoch 14/25, Loss: 0.5615\n",
      "Epoch 15/25: 100%|██████████| 1176/1176 [06:18<00:00,  3.11it/s]\n",
      "2025-10-25 21:58:12,881 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 21:58:34,440 - INFO - Epoch 15/25, Loss: 0.5611\n",
      "Epoch 16/25: 100%|██████████| 1176/1176 [06:03<00:00,  3.24it/s]\n",
      "2025-10-25 22:04:37,719 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 22:04:58,129 - INFO - Epoch 16/25, Loss: 0.5248\n",
      "Epoch 17/25: 100%|██████████| 1176/1176 [06:02<00:00,  3.25it/s]\n",
      "2025-10-25 22:11:00,279 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 22:11:20,863 - INFO - Epoch 17/25, Loss: 0.4404\n",
      "Epoch 18/25: 100%|██████████| 1176/1176 [06:06<00:00,  3.21it/s]\n",
      "2025-10-25 22:17:27,192 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 22:17:47,934 - INFO - Epoch 18/25, Loss: 0.4452\n",
      "Epoch 19/25: 100%|██████████| 1176/1176 [06:08<00:00,  3.19it/s]\n",
      "2025-10-25 22:23:56,698 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 22:24:17,529 - INFO - Epoch 19/25, Loss: 0.4012\n",
      "Epoch 20/25: 100%|██████████| 1176/1176 [06:08<00:00,  3.20it/s]\n",
      "2025-10-25 22:30:25,594 - INFO - Evaluating for early stopping...\n",
      "2025-10-25 22:30:46,769 - INFO - Early stopping triggered at epoch 20.\n",
      "2025-10-25 22:30:46,774 - INFO - Loaded best model state from early stopping.\n"
     ]
    }
   ],
   "source": [
    "train_history = train(\n",
    "    model=har_model,\n",
    "    video_dataset=train_dataset,\n",
    "    device='mps',\n",
    "    epochs=25,\n",
    "    early_stopping=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5bc8d",
   "metadata": {},
   "source": [
    "**Saving Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b74bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 22:30:46,802 - INFO - Saving model to /Volumes/KODAK/masters/model/validation_datasets/NW-UCLA/model/har_model_v0.0.0_nw_ucla_2025-10-25 22:30:46.802204.pht...\n",
      "2025-10-25 22:30:46,908 - INFO - Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "har_model.save(\n",
    "    training_history=train_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4056878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "model_settings = config.model_settings\n",
    "save_dir = \"~/masters-models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_path = os.path.join(\n",
    "    save_dir,\n",
    "    f\"har_model_{model_settings.model_version}_{model_settings.dataset_prefix}_100perc_{datetime.now()}.pht\"\n",
    ")\n",
    "torch.save({\n",
    "\"model_state_dict\": har_model.state_dict(),\n",
    "\"training_history\": train_history,\n",
    "\"model_config\": har_model._model_config,\n",
    "}, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf73a0",
   "metadata": {},
   "source": [
    "## Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f60c1",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a102aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 73.03%\n"
     ]
    }
   ],
   "source": [
    "accuracy_evaluation = evaluate(har_model, test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy_evaluation:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
