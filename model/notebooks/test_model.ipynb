{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b2d9ef",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a5065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config...\n",
      "Loading secrets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:40:49,518 - INFO - Sentry DSN set to: https://f4f21cc936b3ba9f5dbc1464b7a40ea4@o4504168838070272.ingest.us.sentry.io/4506464560414720\n",
      "2025-11-16 12:40:49,519 - INFO - Sentry initialized with environment: development\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "\n",
    "from settings.global_settings import GlobalSettings\n",
    "\n",
    "config = GlobalSettings.get_config(\n",
    "    config_file = \"../config.ini\",\n",
    "    secrets_file = \"../secrets.ini\"\n",
    ")\n",
    "from dataset.video_loader import VideoDataLoader\n",
    "from dataset.video_dataset import VideoDataset, default_augmentation_pipeline\n",
    "from model.training_loop import train, EarlyStoppingParams\n",
    "from model.multimodal_har_model import MultiModalHARModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf9b62",
   "metadata": {},
   "source": [
    "## Initializing Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d691d46",
   "metadata": {},
   "source": [
    "**Creating Dataloaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d84473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:40:51,690 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-11-16 12:40:51,960 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-11-16 12:40:52,327 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-11-16 12:40:52,655 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-11-16 12:40:53,099 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-11-16 12:40:53,360 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-11-16 12:40:53,598 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-11-16 12:40:54,204 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-11-16 12:40:54,740 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-11-16 12:40:54,988 - INFO - [VideoDataLoader] Loding action videos for action: a12\n",
      "2025-11-16 12:40:59,650 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-11-16 12:40:59,690 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-11-16 12:40:59,731 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-11-16 12:40:59,766 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-11-16 12:40:59,810 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-11-16 12:40:59,841 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-11-16 12:40:59,875 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-11-16 12:40:59,950 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-11-16 12:41:00,017 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-11-16 12:41:00,049 - INFO - [VideoDataLoader] Loding action videos for action: a12\n",
      "2025-11-16 12:41:00,563 - INFO - [VideoDataLoader] Loding action videos for action: a01\n",
      "2025-11-16 12:41:00,604 - INFO - [VideoDataLoader] Loding action videos for action: a02\n",
      "2025-11-16 12:41:00,640 - INFO - [VideoDataLoader] Loding action videos for action: a03\n",
      "2025-11-16 12:41:00,670 - INFO - [VideoDataLoader] Loding action videos for action: a04\n",
      "2025-11-16 12:41:00,704 - INFO - [VideoDataLoader] Loding action videos for action: a05\n",
      "2025-11-16 12:41:00,739 - INFO - [VideoDataLoader] Loding action videos for action: a06\n",
      "2025-11-16 12:41:00,769 - INFO - [VideoDataLoader] Loding action videos for action: a08\n",
      "2025-11-16 12:41:00,829 - INFO - [VideoDataLoader] Loding action videos for action: a09\n",
      "2025-11-16 12:41:00,882 - INFO - [VideoDataLoader] Loding action videos for action: a11\n",
      "2025-11-16 12:41:00,921 - INFO - [VideoDataLoader] Loding action videos for action: a12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TRAIN_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"train\"\n",
    ")\n",
    "TEST_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"validation\"\n",
    ")\n",
    "VALIDATION_DIR = os.path.join(\n",
    "    config.model_settings.video_data_dir,\n",
    "    \"test\"\n",
    ")\n",
    "\n",
    "train_video_data_loader = VideoDataLoader(\n",
    "    path=TRAIN_DIR,\n",
    ")\n",
    "test_video_data_loader = VideoDataLoader(\n",
    "    path=TEST_DIR,\n",
    ")\n",
    "validation_video_data_loader = VideoDataLoader(\n",
    "    path=VALIDATION_DIR,\n",
    ")\n",
    "\n",
    "train_dataset = VideoDataset(\n",
    "    video_data_loader=train_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    "    # transform=default_augmentation_pipeline(target_len=16, noise_std=0.02),\n",
    ")\n",
    "test_dataset = VideoDataset(\n",
    "    video_data_loader=test_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "validation_dataset = VideoDataset(\n",
    "    video_data_loader=validation_video_data_loader,\n",
    "    normalization_type=\"across_frames\",\n",
    ")\n",
    "\n",
    "len(train_dataset)\n",
    "for _ in train_dataset:\n",
    "    pass\n",
    "len(train_dataset.labels_map)\n",
    "\n",
    "len(test_dataset)\n",
    "for _ in test_dataset:\n",
    "    pass\n",
    "\n",
    "len(validation_dataset)\n",
    "for _ in validation_dataset:\n",
    "    pass\n",
    "\n",
    "\n",
    "display(len(test_dataset.labels_map))\n",
    "display(len(validation_dataset.labels_map))\n",
    "display(len(train_dataset.labels_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5336f",
   "metadata": {},
   "source": [
    "**Splitting Train and Test Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c82257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_total = len(train_dataset)\n",
    "# num_train = int(0.8 * num_total)\n",
    "# num_test = num_total - num_train\n",
    "# train_dataset, test_dataset = random_split(train_dataset, [num_train, num_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef613137",
   "metadata": {},
   "source": [
    "**Creating Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "781f5692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:41:01,344 - INFO - Model configuration: {'obj_in': 5, 'joint_in': 3, 'gat_hidden': 128, 'gat_out': 128, 'temporal_hidden': 128, 'num_classes': 10, 'dropout': 0.1, 'temporal_pooling': 'attn_pool'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "har_model = MultiModalHARModel(\n",
    "    obj_in=train_dataset[0].graphs_objects[0].x.shape[1],\n",
    "    joint_in=train_dataset[0].graphs_joints[0].x.shape[1],\n",
    "    gat_hidden=128,\n",
    "    gat_out=128,\n",
    "    temporal_hidden=128,\n",
    "    num_classes=len(train_dataset.labels_map), \n",
    "    dropout=0.1,\n",
    "    temporal_pooling=\"attn_pool\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8464c4",
   "metadata": {},
   "source": [
    "**Create Evaluate Function For Early Stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a215b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    import torch\n",
    "    device = 'cpu'\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            sample = dataset[i]\n",
    "            label = sample.label.to(device)\n",
    "\n",
    "            # Move all graph tensors to device\n",
    "            graphs_objects = [g.to(device) for g in sample.graphs_objects]\n",
    "            graphs_joints = [g.to(device) for g in sample.graphs_joints]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(graphs_objects, graphs_joints)\n",
    "\n",
    "            # Compute prediction\n",
    "            if output.dim() == 1:\n",
    "                predicted = torch.argmax(output).unsqueeze(0)\n",
    "            else:\n",
    "                _, predicted = torch.max(output, dim=1)\n",
    "\n",
    "            correct += (predicted == label).sum().item()\n",
    "            total += 1\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3ecc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_params = EarlyStoppingParams(\n",
    "    patience=20,\n",
    "    min_delta=1e-3,\n",
    "    mode='max',\n",
    "    evaluation_function=evaluate,\n",
    "    evaluation_dataset=validation_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdc50bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:41:01,427 - INFO - Starting training loop...\n",
      "2025-11-16 12:41:01,430 - INFO - Using early stopping\n",
      "Epoch 1/70: 100%|██████████| 1176/1176 [01:09<00:00, 16.99it/s]\n",
      "2025-11-16 12:42:10,682 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:42:13,268 - INFO - Epoch 1/70, Loss: 2.0259\n",
      "Epoch 2/70: 100%|██████████| 1176/1176 [01:08<00:00, 17.10it/s]\n",
      "2025-11-16 12:43:22,055 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:43:24,553 - INFO - Epoch 2/70, Loss: 1.2793\n",
      "Epoch 3/70: 100%|██████████| 1176/1176 [01:08<00:00, 17.21it/s]\n",
      "2025-11-16 12:44:32,883 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:44:35,368 - INFO - Epoch 3/70, Loss: 1.0712\n",
      "Epoch 4/70: 100%|██████████| 1176/1176 [01:09<00:00, 16.81it/s]\n",
      "2025-11-16 12:45:45,321 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:45:47,807 - INFO - Epoch 4/70, Loss: 0.8856\n",
      "Epoch 5/70: 100%|██████████| 1176/1176 [01:08<00:00, 17.24it/s]\n",
      "2025-11-16 12:46:56,039 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:46:58,498 - INFO - Epoch 5/70, Loss: 0.7186\n",
      "Epoch 6/70: 100%|██████████| 1176/1176 [01:14<00:00, 15.70it/s]\n",
      "2025-11-16 12:48:13,404 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:48:15,956 - INFO - Epoch 6/70, Loss: 0.6163\n",
      "Epoch 7/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.53it/s]\n",
      "2025-11-16 12:49:31,706 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:49:34,217 - INFO - Epoch 7/70, Loss: 0.4974\n",
      "Epoch 8/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.44it/s]\n",
      "2025-11-16 12:50:50,404 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:50:53,008 - INFO - Epoch 8/70, Loss: 0.4139\n",
      "Epoch 9/70: 100%|██████████| 1176/1176 [01:18<00:00, 14.90it/s]\n",
      "2025-11-16 12:52:11,951 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:52:14,493 - INFO - Epoch 9/70, Loss: 0.3636\n",
      "Epoch 10/70: 100%|██████████| 1176/1176 [01:14<00:00, 15.74it/s]\n",
      "2025-11-16 12:53:29,188 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:53:31,716 - INFO - Epoch 10/70, Loss: 0.2986\n",
      "Epoch 11/70: 100%|██████████| 1176/1176 [01:11<00:00, 16.36it/s]\n",
      "2025-11-16 12:54:43,588 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:54:46,211 - INFO - Epoch 11/70, Loss: 0.2306\n",
      "Epoch 12/70: 100%|██████████| 1176/1176 [01:13<00:00, 16.04it/s]\n",
      "2025-11-16 12:55:59,535 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:56:02,126 - INFO - Epoch 12/70, Loss: 0.1911\n",
      "Epoch 13/70: 100%|██████████| 1176/1176 [01:13<00:00, 15.90it/s]\n",
      "2025-11-16 12:57:16,084 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:57:18,587 - INFO - Epoch 13/70, Loss: 0.1851\n",
      "Epoch 14/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.27it/s]\n",
      "2025-11-16 12:58:35,583 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:58:38,326 - INFO - Epoch 14/70, Loss: 0.1700\n",
      "Epoch 15/70: 100%|██████████| 1176/1176 [01:17<00:00, 15.17it/s]\n",
      "2025-11-16 12:59:55,859 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 12:59:58,404 - INFO - Epoch 15/70, Loss: 0.1042\n",
      "Epoch 16/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.35it/s]\n",
      "2025-11-16 13:01:15,004 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:01:17,593 - INFO - Epoch 16/70, Loss: 0.1324\n",
      "Epoch 17/70: 100%|██████████| 1176/1176 [01:18<00:00, 15.03it/s]\n",
      "2025-11-16 13:02:35,849 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:02:38,452 - INFO - Epoch 17/70, Loss: 0.1281\n",
      "Epoch 18/70: 100%|██████████| 1176/1176 [01:17<00:00, 15.10it/s]\n",
      "2025-11-16 13:03:56,335 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:03:59,258 - INFO - Epoch 18/70, Loss: 0.1488\n",
      "Epoch 19/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.32it/s]\n",
      "2025-11-16 13:05:16,042 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:05:18,743 - INFO - Epoch 19/70, Loss: 0.1281\n",
      "Epoch 20/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.29it/s]\n",
      "2025-11-16 13:06:35,652 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:06:38,291 - INFO - Epoch 20/70, Loss: 0.0947\n",
      "Epoch 21/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.28it/s]\n",
      "2025-11-16 13:07:55,264 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:07:57,836 - INFO - Epoch 21/70, Loss: 0.1029\n",
      "Epoch 22/70: 100%|██████████| 1176/1176 [01:17<00:00, 15.19it/s]\n",
      "2025-11-16 13:09:15,266 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:09:17,811 - INFO - Epoch 22/70, Loss: 0.0645\n",
      "Epoch 23/70: 100%|██████████| 1176/1176 [01:17<00:00, 15.20it/s]\n",
      "2025-11-16 13:10:35,163 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:10:37,756 - INFO - Epoch 23/70, Loss: 0.1519\n",
      "Epoch 24/70: 100%|██████████| 1176/1176 [01:17<00:00, 15.25it/s]\n",
      "2025-11-16 13:11:54,878 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:11:57,481 - INFO - Epoch 24/70, Loss: 0.0930\n",
      "Epoch 25/70: 100%|██████████| 1176/1176 [01:18<00:00, 15.07it/s]\n",
      "2025-11-16 13:13:15,519 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:13:18,079 - INFO - Epoch 25/70, Loss: 0.0595\n",
      "Epoch 26/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.32it/s]\n",
      "2025-11-16 13:14:34,865 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:14:37,312 - INFO - Epoch 26/70, Loss: 0.0910\n",
      "Epoch 27/70: 100%|██████████| 1176/1176 [16:45<00:00,  1.17it/s]  \n",
      "2025-11-16 13:31:22,946 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:31:25,408 - INFO - Epoch 27/70, Loss: 0.0833\n",
      "Epoch 28/70: 100%|██████████| 1176/1176 [04:19<00:00,  4.52it/s] \n",
      "2025-11-16 13:35:45,306 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:35:47,872 - INFO - Epoch 28/70, Loss: 0.0430\n",
      "Epoch 29/70: 100%|██████████| 1176/1176 [09:46<00:00,  2.01it/s] \n",
      "2025-11-16 13:45:34,125 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:45:36,604 - INFO - Epoch 29/70, Loss: 0.1180\n",
      "Epoch 30/70: 100%|██████████| 1176/1176 [01:35<00:00, 12.35it/s]\n",
      "2025-11-16 13:47:11,830 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:47:15,792 - INFO - Epoch 30/70, Loss: 0.0553\n",
      "Epoch 31/70: 100%|██████████| 1176/1176 [01:17<00:00, 15.18it/s]\n",
      "2025-11-16 13:48:33,274 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:48:35,703 - INFO - Epoch 31/70, Loss: 0.0474\n",
      "Epoch 32/70: 100%|██████████| 1176/1176 [01:14<00:00, 15.73it/s]\n",
      "2025-11-16 13:49:50,477 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:49:52,960 - INFO - Epoch 32/70, Loss: 0.1010\n",
      "Epoch 33/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.59it/s]\n",
      "2025-11-16 13:51:08,391 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:51:10,859 - INFO - Epoch 33/70, Loss: 0.0686\n",
      "Epoch 34/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.50it/s]\n",
      "2025-11-16 13:52:26,751 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:52:29,227 - INFO - Epoch 34/70, Loss: 0.0803\n",
      "Epoch 35/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.66it/s]\n",
      "2025-11-16 13:53:44,329 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:53:46,803 - INFO - Epoch 35/70, Loss: 0.0642\n",
      "Epoch 36/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.54it/s]\n",
      "2025-11-16 13:55:02,465 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:55:04,929 - INFO - Epoch 36/70, Loss: 0.0664\n",
      "Epoch 37/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.66it/s]\n",
      "2025-11-16 13:56:20,043 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:56:22,517 - INFO - Epoch 37/70, Loss: 0.0655\n",
      "Epoch 38/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.68it/s]\n",
      "2025-11-16 13:57:37,543 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:57:40,023 - INFO - Epoch 38/70, Loss: 0.0197\n",
      "Epoch 39/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.43it/s]\n",
      "2025-11-16 13:58:56,223 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 13:58:58,710 - INFO - Epoch 39/70, Loss: 0.0093\n",
      "Epoch 40/70: 100%|██████████| 1176/1176 [15:47<00:00,  1.24it/s]   \n",
      "2025-11-16 14:14:46,560 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:14:49,004 - INFO - Epoch 40/70, Loss: 0.1611\n",
      "Epoch 41/70: 100%|██████████| 1176/1176 [01:16<00:00, 15.43it/s]\n",
      "2025-11-16 14:16:05,207 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:16:07,688 - INFO - Epoch 41/70, Loss: 0.0787\n",
      "Epoch 42/70: 100%|██████████| 1176/1176 [01:14<00:00, 15.70it/s]\n",
      "2025-11-16 14:17:22,597 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:17:25,075 - INFO - Epoch 42/70, Loss: 0.0329\n",
      "Epoch 43/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.65it/s]\n",
      "2025-11-16 14:18:40,230 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:18:42,691 - INFO - Epoch 43/70, Loss: 0.0662\n",
      "Epoch 44/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.54it/s]\n",
      "2025-11-16 14:19:58,351 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:20:00,838 - INFO - Epoch 44/70, Loss: 0.0359\n",
      "Epoch 45/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.65it/s]\n",
      "2025-11-16 14:21:15,977 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:21:18,473 - INFO - Epoch 45/70, Loss: 0.0107\n",
      "Epoch 46/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.68it/s]\n",
      "2025-11-16 14:22:33,489 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:22:35,968 - INFO - Epoch 46/70, Loss: 0.0787\n",
      "Epoch 47/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.58it/s]\n",
      "2025-11-16 14:23:51,432 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:23:53,928 - INFO - Epoch 47/70, Loss: 0.0385\n",
      "Epoch 48/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.59it/s]\n",
      "2025-11-16 14:25:09,357 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:25:11,969 - INFO - Epoch 48/70, Loss: 0.0802\n",
      "Epoch 49/70: 100%|██████████| 1176/1176 [01:15<00:00, 15.63it/s]\n",
      "2025-11-16 14:26:27,194 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:41:55,625 - INFO - Epoch 49/70, Loss: 0.0576\n",
      "Epoch 50/70: 100%|██████████| 1176/1176 [13:06<00:00,  1.50it/s] \n",
      "2025-11-16 14:55:01,928 - INFO - Evaluating for early stopping...\n",
      "2025-11-16 14:55:04,469 - INFO - Early stopping triggered at epoch 50.\n",
      "2025-11-16 14:55:04,493 - INFO - Loaded best model state from early stopping.\n"
     ]
    }
   ],
   "source": [
    "from model.training_loop import WarmupSchedulerParams\n",
    "\n",
    "\n",
    "train_history = train(\n",
    "    model=har_model,\n",
    "    video_dataset=train_dataset,\n",
    "    device='cpu',\n",
    "    epochs=70,\n",
    "    lr=1e-4,\n",
    "    early_stopping=early_stopping_params,\n",
    "    warmup_scheduler_params=WarmupSchedulerParams(True, 800)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cabcd9",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2e009",
   "metadata": {},
   "source": [
    "**Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6abf28",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd5bc8d",
   "metadata": {},
   "source": [
    "**Saving Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b74bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:55:04,723 - INFO - Saving model to /Volumes/KODAK/masters/model/validation_datasets/NW-UCLA/model/har_model_v1.0.0_nw_ucla_20251116_145504.pht...\n",
      "2025-11-16 14:55:05,644 - INFO - Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "har_model.save(\n",
    "    training_history=train_history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf73a0",
   "metadata": {},
   "source": [
    "## Running tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f60c1",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a102aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 80.26%\n"
     ]
    }
   ],
   "source": [
    "accuracy_evaluation = evaluate(har_model, test_dataset)\n",
    "print(f\"Test Accuracy: {accuracy_evaluation:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d4d4d",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f9a2abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape torch.Size([1, 10])\n",
      "out device cpu label device cpu\n",
      "logits: [[  9.104175    2.1412585  -0.6019546  -7.237774   -4.6213875  -0.2401329\n",
      "  -17.461979  -14.522748   -9.532532  -13.011941 ]]\n",
      "probs: [[9.9890530e-01 9.4529678e-04 6.0842354e-05 7.9855688e-08 1.0929202e-06\n",
      "  8.7366097e-05 2.8972768e-12 5.4762365e-11 8.0483185e-09 2.4809463e-10]]\n",
      "entropy: 0.00910070352256298\n",
      "pred: 0 label: 0\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "device = 'cpu'   # match training device\n",
    "har_model.to(device)\n",
    "har_model.eval()\n",
    "\n",
    "sample = train_dataset[0]\n",
    "graphs_objects = [g.to(device) for g in sample.graphs_objects]\n",
    "graphs_joints = [g.to(device) for g in sample.graphs_joints]\n",
    "label = sample.label.unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = har_model(graphs_objects, graphs_joints)   # expect [1, num_classes]\n",
    "    probs = F.softmax(out, dim=-1)\n",
    "    ent = -(probs * probs.log()).sum(dim=-1)     # entropy\n",
    "    pred = torch.argmax(probs, dim=-1)\n",
    "\n",
    "print(\"out.shape\", out.shape)\n",
    "print(\"out device\", out.device, \"label device\", label.device)\n",
    "print(\"logits:\", out.cpu().numpy())\n",
    "print(\"probs:\", probs.cpu().numpy())\n",
    "print(\"entropy:\", ent.item())\n",
    "print(\"pred:\", pred.item(), \"label:\", label.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7199e",
   "metadata": {},
   "source": [
    "**Mapping Consistency - Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da92364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN ===\n",
      "len: 1176\n",
      "unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "counts: Counter({3: 138, 5: 119, 0: 118, 4: 118, 1: 116, 8: 116, 6: 113, 7: 113, 9: 113, 2: 112})\n",
      "labels_map (sample): {'a01': 0, 'a02': 1, 'a03': 2, 'a04': 3, 'a05': 4, 'a06': 5, 'a08': 6, 'a09': 7, 'a11': 8, 'a12': 9}\n",
      "\n",
      "=== VAL ===\n",
      "len: 147\n",
      "unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "counts: Counter({3: 17, 0: 15, 1: 15, 4: 15, 5: 15, 2: 14, 6: 14, 7: 14, 8: 14, 9: 14})\n",
      "labels_map (sample): {'a01': 0, 'a02': 1, 'a03': 2, 'a04': 3, 'a05': 4, 'a06': 5, 'a08': 6, 'a09': 7, 'a11': 8, 'a12': 9}\n",
      "\n",
      "=== TEST ===\n",
      "len: 152\n",
      "unique labels: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "counts: Counter({3: 18, 0: 15, 1: 15, 4: 15, 5: 15, 6: 15, 7: 15, 8: 15, 9: 15, 2: 14})\n",
      "labels_map (sample): {'a01': 0, 'a02': 1, 'a03': 2, 'a04': 3, 'a05': 4, 'a06': 5, 'a08': 6, 'a09': 7, 'a11': 8, 'a12': 9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def inspect_dataset(dataset, name):\n",
    "    labels = [int(s.label) for s in dataset]\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(\"len:\", len(labels))\n",
    "    print(\"unique labels:\", sorted(set(labels)))\n",
    "    print(\"counts:\", Counter(labels))\n",
    "    print(\"labels_map (sample):\", getattr(dataset, \"labels_map\", None))\n",
    "    print()\n",
    "\n",
    "inspect_dataset(train_dataset, \"TRAIN\")\n",
    "inspect_dataset(validation_dataset, \"VAL\")\n",
    "inspect_dataset(test_dataset, \"TEST\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a023a48",
   "metadata": {},
   "source": [
    "**Prediction Distribution - Predicts only few classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac297c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred distribution: Counter({8: 19, 7: 18, 1: 16, 3: 16, 0: 15, 5: 15, 9: 14, 2: 13, 4: 13, 6: 8})\n",
      "true distribution : Counter({3: 17, 0: 15, 1: 15, 4: 15, 5: 15, 2: 14, 6: 14, 7: 14, 8: 14, 9: 14})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import torch, torch.nn.functional as F\n",
    "\n",
    "def pred_distribution(model, dataset, device='cpu'):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    truths = []\n",
    "    with torch.no_grad():\n",
    "        for s in dataset:\n",
    "            graphs_objects = [g.to(device) for g in s.graphs_objects]\n",
    "            graphs_joints  = [g.to(device) for g in s.graphs_joints]\n",
    "            out = model(graphs_objects, graphs_joints)  # [1, C]\n",
    "            preds.append(int(torch.argmax(out, dim=-1)))\n",
    "            truths.append(int(s.label))\n",
    "    print(\"pred distribution:\", Counter(preds))\n",
    "    print(\"true distribution :\", Counter(truths))\n",
    "    return preds, truths\n",
    "\n",
    "preds, truths = pred_distribution(har_model, validation_dataset, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd34ac",
   "metadata": {},
   "source": [
    "**Confusion Matrix - Per Class Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edb7bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[14  0  1  0  0  0  0  0  0  0]\n",
      " [ 1 14  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  9  2  0  0  0  0  2  1]\n",
      " [ 0  0  2 12  0  0  0  0  2  1]\n",
      " [ 0  1  0  0 13  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 15  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  7  5  1  0]\n",
      " [ 0  0  0  1  0  0  0 11  2  0]\n",
      " [ 0  0  1  0  0  0  0  2  9  2]\n",
      " [ 0  0  0  1  0  0  0  0  3 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9333    0.9333    0.9333        15\n",
      "           1     0.8750    0.9333    0.9032        15\n",
      "           2     0.6923    0.6429    0.6667        14\n",
      "           3     0.7500    0.7059    0.7273        17\n",
      "           4     1.0000    0.8667    0.9286        15\n",
      "           5     1.0000    1.0000    1.0000        15\n",
      "           6     0.8750    0.5000    0.6364        14\n",
      "           7     0.6111    0.7857    0.6875        14\n",
      "           8     0.4737    0.6429    0.5455        14\n",
      "           9     0.7143    0.7143    0.7143        14\n",
      "\n",
      "    accuracy                         0.7755       147\n",
      "   macro avg     0.7925    0.7725    0.7743       147\n",
      "weighted avg     0.7959    0.7755    0.7779       147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(truths, preds)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(classification_report(truths, preds, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
